{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "post-training-quant.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "6Y8E0lw5eYWm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Post Training Quantization"
      ]
    },
    {
      "metadata": {
        "id": "CIGrZZPTZVeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tutorials/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tutorials/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "BTC1rDAuei_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports\n",
        "converting weights to 8 bit precision as part of model conversion from\n",
        "tensorflow graphdefs to TFLite's flat buffer format. Weight quantization\n",
        "achieves a 4x reduction in the model size. In addition, TFLite supports on the\n",
        "fly quantization and dequantization of activations to allow for:\n",
        "\n",
        "1.  Using quantized kernels for faster implementation when available.\n",
        "\n",
        "2.  Mixing of floating-point kernels with quantized kernels for different parts\n",
        "    of the graph.\n",
        "\n",
        "Note that the activations are always stored in floating point. For ops that\n",
        "support quantized kernels, the activations are quantized to 8 bits of precision\n",
        "dynamically prior to processing and are de-quantized to float precision after\n",
        "processing. Depending on the model being converted, this can give a speedup over\n",
        "pure floating point computation.\n",
        "\n",
        "In contrast to\n",
        "[quantization aware training](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize)\n",
        ", the weights are quantized post training and the activations are quantized dynamically \n",
        "at inference in this method.\n",
        "Therefore, the model weights are not retrained to compensate for quantization\n",
        "induced errors. It is important to check the accuracy of the quantized model to\n",
        "ensure that the degradation is acceptable.\n",
        "\n",
        "In this tutorial, we train an MNIST model from scratch, check its accuracy in\n",
        "tensorflow and then convert the saved model into a Tensorflow Lite flatbuffer\n",
        "with weight quantization. We finally check the\n",
        "accuracy of the converted model and compare it to the original saved model. We\n",
        "run the training script mnist.py from\n",
        "[Tensorflow official mnist tutorial](https://github.com/tensorflow/models/tree/master/official/mnist).\n"
      ]
    },
    {
      "metadata": {
        "id": "2XsEP17Zelz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building an MNIST model"
      ]
    },
    {
      "metadata": {
        "id": "dDqqUIZjZjac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "gyqAw1M9lyab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip install -U tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WsN6s5L1ieNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00U0taBoe-w7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4XZPtSh-fUOc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if sys.version_info.major >= 3:\n",
        "    import pathlib\n",
        "else:\n",
        "    import pathlib2 as pathlib\n",
        "\n",
        "# Add `models` to the python path.\n",
        "models_path = os.path.join(os.getcwd(), \"models\")\n",
        "sys.path.append(models_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQ6Q0qqKZogR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and export the model"
      ]
    },
    {
      "metadata": {
        "id": "eMsw_6HujaqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saved_models_root = \"/tmp/mnist_saved_model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hWSAjQWagIHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The above path addition is not visible to subprocesses, add the path for the subprocess as well.\n",
        "# Note: channels_last is required here or the conversion may fail. \n",
        "!PYTHONPATH={models_path} python models/official/mnist/mnist.py --train_epochs=1 --export_dir {saved_models_root} --data_format=channels_last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NMaNZQCkW9X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the example, we only trained the model for a single epoch, so it only trains to ~96% accuracy.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xl8_fzVAZwOh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert to a TFLite model\n",
        "\n",
        "The `savedmodel` directory is named with a timestamp. Select the most recent one: "
      ]
    },
    {
      "metadata": {
        "id": "Xp5oClaZkbtn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saved_model_dir = str(sorted(pathlib.Path(saved_models_root).glob(\"*\"))[-1])\n",
        "saved_model_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AT8BgkKmljOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the python `TocoConverter`, the saved model can be converted into a TFLite model.\n",
        "\n",
        "First load the model using the `TocoConverter`:"
      ]
    },
    {
      "metadata": {
        "id": "_i8B2nDZmAgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2o2ZfF0aiCx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write it out to a tflite file:"
      ]
    },
    {
      "metadata": {
        "id": "vptWZq2xnclo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ie9pQaQrn5ue",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BONhYtYocQY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To quantize the model on export, set the `post_training_quantize` flag:"
      ]
    },
    {
      "metadata": {
        "id": "g8PUvLWDlmmz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note: If you don't have a recent tf-nightly installed, the\n",
        "# \"post_training_quantize\" line will have no effect.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "converter.post_training_quantize = True\n",
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_quant_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PhMmUTl4sbkz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note how the resulting file, with `post_training_quantize` set, is approximately `1/4` the size."
      ]
    },
    {
      "metadata": {
        "id": "JExfcfLDscu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L8lQHMp_asCq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run the TFLite models"
      ]
    },
    {
      "metadata": {
        "id": "-5l6-ciItvX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can run the TensorFlow Lite model using the python TensorFlow Lite\n",
        "Interpreter. \n",
        "\n",
        "### load the test data\n",
        "\n",
        "First let's load the mnist test data to feed to it:"
      ]
    },
    {
      "metadata": {
        "id": "eTIuU07NuKFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
        "images, labels = tf.to_float(mnist_test[0])/255.0, mnist_test[1]\n",
        "\n",
        "# Note: If you change the batch size, then use \n",
        "# `tf.contrib.lite.Interpreter.resize_tensor_input` to also change it for\n",
        "# the interpreter.\n",
        "mnist_ds = tf.data.Dataset.from_tensor_slices((images, labels)).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ap_jE7QRvhPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the model into an interpreter"
      ]
    },
    {
      "metadata": {
        "id": "Jn16Rc23zTss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "interpreter = tf.contrib.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8Pztk1mvNVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "interpreter_quant = tf.contrib.lite.Interpreter(model_path=str(tflite_model_quant_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Afl6yGvWyqAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "interpreter_quant.allocate_tensors()\n",
        "input_index = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter_quant.get_output_details()[0][\"index\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2opUt_JTdyEu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test the model on one image"
      ]
    },
    {
      "metadata": {
        "id": "AKslvo2kwWac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for img, label in mnist_ds.take(1):\n",
        "  break\n",
        "\n",
        "interpreter.set_tensor(input_index, img)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(output_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZClM2vo3_bm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.imshow(img[0])\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true= str(label[0].numpy()),\n",
        "                              predict=str(predictions[0,0])))\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LwN7uIdCd8Gw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the models"
      ]
    },
    {
      "metadata": {
        "id": "05aeAuWjvjPx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_model(interpreter, mnist_ds):\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for img, label in mnist_ds:\n",
        "    total_seen += 1\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(output_index)\n",
        "    if predictions == label.numpy():\n",
        "      num_correct += 1\n",
        "\n",
        "    if total_seen % 500 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "\n",
        "  return float(num_correct) / float(total_seen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqXBnDfJ7qxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(eval_model(interpreter, mnist_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Km3cY9ry8ZlG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can repeat the evaluation on the weight quantized model to obtain:\n"
      ]
    },
    {
      "metadata": {
        "id": "-9cnwiPp6EGm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(eval_model(interpreter_quant, mnist_ds))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7lfxkor8pgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we have compressed model with no difference in the accuracy."
      ]
    },
    {
      "metadata": {
        "id": "M0o1FtmWeKZm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Optimizing an existing model\n",
        "\n",
        "We now consider another example. Resnets with pre-activation layers (Resnet-v2) are widely used for vision applications.\n",
        "  Pre-trained frozen graph for resnet-v2-101 is available at the\n",
        "  [Tensorflow Lite model repository](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md).\n",
        "\n",
        "We can convert the frozen graph to a TFLite flatbuffer with quantization by:\n"
      ]
    },
    {
      "metadata": {
        "id": "v5p5VcNPjILQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "archive_path = tf.keras.utils.get_file(\"resnet_v2_101.tgz\", \"https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/resnet_v2_101.tgz\", extract=True)\n",
        "archive_path = pathlib.Path(archive_path)\n",
        "archive_dir = str(archive_path.parent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sxnXQuC4ThD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `info.txt` file lists the input and output names. You can also find them using TensorBoard to visually inspect the graph."
      ]
    },
    {
      "metadata": {
        "id": "g_Q_OMEJ4LIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cat {archive_dir}/resnet_v2_101_299_info.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujCAFhqm-C6H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "graph_def_file = pathlib.Path(archive_path).parent/\"resnet_v2_101_299_frozen.pb\"\n",
        "input_arrays = [\"input\"] \n",
        "output_arrays = [\"output\"]\n",
        "converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n",
        "  str(graph_def_file), input_arrays, output_arrays, input_shapes={\"input\":[1,299,299,3]})\n",
        "converter.post_training_quantize = True\n",
        "resnet_tflite_file = graph_def_file.parent/\"resnet_v2_101_quantized.tflite\"\n",
        "resnet_tflite_file.write_bytes(converter.convert())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vhOjeg1x9Knp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!ls -lh {archive_dir}/*.tflite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qqHLaqFMCjRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model size reduces from 171 MB to 43 MB.\n",
        "The accuracy of this model on imagenet can be evaluated using the scripts provided for [TFLite accuracy measurement](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/accuracy/ilsvrc).\n",
        "\n",
        "The optimized model top-1 accuracy is 76.8, the same as the floating point model."
      ]
    }
  ]
}